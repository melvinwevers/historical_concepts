import glob
import pandas as pd
import re
import nltk
import gensim
import logging
import random
#from gensim.models import FastText
#from gensim.similarities.index import AnnoyIndexer
from gensim.models import Phrases
from gensim.models.phrases import Phraser
from gensim.models.word2vec import LineSentence
import glob
import os
from gensim.models import KeyedVectors

#from functools32 import lru_cache

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

path = '../data/newspapers/vk'


allFiles = glob.glob(path + "/*.tsv")
#print(allFiles)
tokenizer = nltk.data.load('tokenizers/punkt/dutch.pickle')

def cleaning(text):
    text = re.sub("[^a-zA-Z]"," ", text)
    words = text.lower().split()
    return(words)

def article_to_sentences(text):
    #raw_sentences = tokenizer.tokenize(text.strip())
    raw_sentences = text.split('.')
    sentences = []
    for raw_sentence in raw_sentences:
        if len(raw_sentence) > 0:
            sentences.append(cleaning(raw_sentence))
    return sentences

df = pd.concat((pd.read_csv(f, delimiter='\t', header=None) for f in allFiles))
df.columns = ['date', 'page', 'size', 'min_x', 'min_y', 'max_x', 'max_y', 'w', 'h', 'image_url', 'ocr_url', 'ocr']
df = df.dropna(subset =['ocr']) # remove lines with empty ocr field


df = df[~df['date'].str.contains('date')] #remove duplicate header rows
# remove files that contain error msg 
excludes = ['objecttype', 'file directory not found']
df = df[~df['ocr'].astype(str).str.contains('|'.join(excludes))] #remove 
df['date'] = pd.to_datetime(df['date'])
df['year'] = df['date'].dt.year
for name, group in df.groupby('year'):
    print('making sentences: {}'.format(name))
    sentences = group['ocr'].apply(article_to_sentences)
    sentences.to_csv(str(name) + 'sentences', index=False, sep='\t', encoding='utf-8')
