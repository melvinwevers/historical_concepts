{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import logging\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "import numpy as np\n",
    "import csv\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/dutch.pickle')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/newspapers/test'\n",
    "path2 = '../data/sentences/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles = glob.glob(path2 + \"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train online\n",
    "\n",
    "model = gensim.models.Word2Vec(min_count=50, size = 200, iter=10, window = 10, workers = 5, sg=1)\n",
    "counter = 0\n",
    "for file in allFiles:\n",
    "    print(counter)\n",
    "    sentences = gensim.models.word2vec.LineSentence(file)\n",
    "    bigram_transformer = gensim.models.Phrases(sentences)\n",
    "    bigram = gensim.models.phrases.Phraser(bigram_transformer)\n",
    "    corpus = list(bigram[sentences])\n",
    "    if counter == 0:\n",
    "        model.build_vocab(corpus)\n",
    "    else:\n",
    "        model.build_vocab(corpus, update=True)\n",
    "    counter += 1\n",
    "model.train(corpus , total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentencesForYear(year):\n",
    "    corpus = []\n",
    "    for file_ in allFiles:\n",
    "        filename = os.path.basename(file_)\n",
    "        if filename.startswith(str(year)):\n",
    "            sentences = gensim.models.word2vec.LineSentence(file)      \n",
    "    return sentences\n",
    "\n",
    "def getSentencesInRange(startY, endY):\n",
    "    return [s for year in range(startY, endY) for s in getSentencesForYear(year)]    \n",
    "\n",
    "def train_models():\n",
    "    model = gensim.models.Word2Vec(min_count=50, size = 200, iter=10, window = 10, workers = 5, sg=1)\n",
    "    \n",
    "    yearsInModel = 1    \n",
    "    stepYears = 1\n",
    "    modelFolder = '../models'\n",
    "\n",
    "    y0 = 1950\n",
    "    yN = 1956\n",
    "\n",
    "    for year in range(y0, yN-yearsInModel+1, stepYears):\n",
    "        startY = year\n",
    "        endY = year + yearsInModel\n",
    "        modelName = modelFolder + '/%d_%d.w2v'%(year,year+yearsInModel)\n",
    "        print('Building Model: ', modelName)\n",
    "\n",
    "        sentences = getSentencesInRange(startY, endY)\n",
    "        bigram_transformer = gensim.models.Phrases(sentences)\n",
    "        bigram = gensim.models.phrases.Phraser(bigram_transformer)\n",
    "        corpus = list(bigram[sentences])\n",
    "        \n",
    "        model.build_vocab(corpus)\n",
    "        model.train(corpus , total_examples=model.corpus_count, epochs=model.iter)\n",
    "        print('....saving')\n",
    "        model.init_sims(replace=True)\n",
    "        model.wv.save_word2vec_format(modelName, binary=True)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('../models/1950_1951.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('../models/1950_1951.w2v', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(min_count=50, size = 200, iter=5, alpha=0.025, window = 10, workers = 5, sg=0)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus , total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"verenigde_staten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('amerika','verenigde_staten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('amerika', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = ['verenigde_staten','china','japan']\n",
    "model.most_similar_to_given('amerika', candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in candidates:\n",
    "    print(c, model.similarity('amerika',c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.words_closer_than('koud','droog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [\"china\", \"rusland\", \"frankrijk\", \"duitsland\"]\n",
    "capitals = [\"peking\",\"moskou\",\"parijs\",\"berlijn\"]\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for loc in countries+capitals:\n",
    "    X.append(model[loc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "xy_coords = pca.fit_transform(X)\n",
    "loc_x, loc_y = zip(*xy_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.scatter(loc_x, loc_y)\n",
    "\n",
    "for _, location in enumerate(countries+capitals):\n",
    "    ax.annotate(location, (loc_x[_]+.05, loc_y[_]-.05))\n",
    "\n",
    "plt.title(\"Countries and their Capitals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = ['sieraden', 'natuurkunde', 'aardig', 'genie', 'leider', 'karakter',  \n",
    "                'zaken', 'rijk', 'wapen', 'gek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "she = model['zij'].reshape(1,-1)\n",
    "he = model['hij'].reshape(1,-1)\n",
    "\n",
    "for word in descriptions:\n",
    "    our_vector = model[word].reshape(1,-1)\n",
    "    print(word+\"_she\", cosine_similarity(our_vector, she))\n",
    "    print(word+\"_he\", cosine_similarity(our_vector, he)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(positive=['vrouw','professor'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(positive=['ali','dokter'], negative=['mark'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(positive=['zij','dief'], negative=['hij'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import debiaswe as dwe\n",
    "import debiaswe.we as we\n",
    "from debiaswe.we import WordEmbedding\n",
    "from debiaswe.data import load_professions\n",
    "import numpy.linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load debias.py\n",
    "from __future__ import print_function, division\n",
    "import we\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    import io\n",
    "    open = io.open\n",
    "\"\"\"\n",
    "Hard-debias embedding\n",
    "\n",
    "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\n",
    "Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai\n",
    "2016\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def debias(E, gender_specific_words, definitional, equalize):\n",
    "    gender_direction = we.doPCA(definitional, E).components_[0]\n",
    "    specific_set = set(gender_specific_words)\n",
    "    for i, w in enumerate(E.words):\n",
    "        if w not in specific_set:\n",
    "            E.vecs[i] = we.drop(E.vecs[i], gender_direction)\n",
    "    E.normalize()\n",
    "    candidates = {x for e1, e2 in equalize for x in [(e1.lower(), e2.lower()),\n",
    "                                                     (e1.title(), e2.title()),\n",
    "                                                     (e1.upper(), e2.upper())]}\n",
    "    print(candidates)\n",
    "    for (a, b) in candidates:\n",
    "        if (a in E.index and b in E.index):\n",
    "            y = we.drop((E.v(a) + E.v(b)) / 2, gender_direction)\n",
    "            z = np.sqrt(1 - np.linalg.norm(y)**2)\n",
    "            if (E.v(a) - E.v(b)).dot(gender_direction) < 0:\n",
    "                z = -z\n",
    "            E.vecs[E.index[a]] = z * gender_direction + y\n",
    "            E.vecs[E.index[b]] = -z * gender_direction + y\n",
    "    E.normalize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
